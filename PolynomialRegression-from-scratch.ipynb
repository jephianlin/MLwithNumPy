{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolynomialRegression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "**Input:**  \n",
    "- `X`: an array of shape `(N,1)` whose rows are samples and columns are features\n",
    "- `y`: the labels of shape `(N,)`\n",
    "- `degree`: the degree of the polynomial \n",
    "- `**kwargs`: keywords for your linear regression function\n",
    "\n",
    "**Output:**  \n",
    "Revised output of your linear regression function.\n",
    "\n",
    "**Steps:**\n",
    "1. Let `X_ex = X**np.arange(1, degree + 1)` .\n",
    "2. Suppose `LR` is your linear regression fuction.  \n",
    "Let `predict_lin,coef,intercept = LR(X_ex, y, **kwargs)` .  \n",
    "3. Define the function `predict` that sends `X_test` to `(X_test**np.arange(1, degree+1)).dot(coef) + intercept` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "Translate the algorithm into the pseudocode.  \n",
    "This helps you to identify the parts that you don't know how to do it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. \n",
    "    2. \n",
    "    3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Take some sample data from [PolynomialRegression-with-scikit-learn](PolynomialRegression-with-scikit-learn.ipynb) and check if your code generates similar outputs with the existing packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Name of the data\n",
    "Description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with existing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "Let  \n",
    "```python\n",
    "degree = 3\n",
    "x = np.arange(5)\n",
    "X = x[:,np.newaxis]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(a)\n",
    "Let `X_ex1 = X**np.arange(1, degree+1)` .  \n",
    "The new data `X_ex1` is supposed to be the same as the output of `sklearn.preprocessing.PolynomialFeatures` with `include_bias=False` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(b)\n",
    "Let `X_ex1 = X**np.arange(0, degree+1)` .  \n",
    "The new data `X_ex1` is supposed to be the same as the output of `sklearn.preprocessing.PolynomialFeatures` with `include_bias=False` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(a)\n",
    "Let `degree=2` .\n",
    "Apply the linear regresssion algorithm to `X`  \n",
    "1. by your code with `algorithm==\"projection\"` ,  \n",
    "2. by your code with `algorithm==\"grad_descent\"` ,  \n",
    "3. by `sklearn.linear_model.LinearRegresssion` .  \n",
    "\n",
    "Check if the outputs are almost the same (up to some numerical errors).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(b)\n",
    "Modify your code so that it prints the mean square error at each step of the gradient descent.  \n",
    "Check if it is always decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "Add a new keyword `regularization`, which can be `None`, `\"L1\"`, or `\"L2\"` .  \n",
    "Add another keyword `alpha`, which is a positive number.  \n",
    "\n",
    "When `regularization==None`, the cost function is \n",
    "$$\\frac{1}{N}\\sum_{i=0}^{N-1}\\|f({\\bf x}_i) - y_i\\|^2.$$ \n",
    "When `regularization==\"L1\"`, the cost function is \n",
    "$$\\frac{1}{N}\\sum_{i=0}^{N-1}\\|f({\\bf x}_i) - y_i\\|^2 + \\alpha\\sum_{i=0}^{d-1}|c_i|.$$ \n",
    "When `regularization==\"L2\"`, the cost function is \n",
    "$$\\frac{1}{N}\\sum_{i=0}^{N-1}\\|f({\\bf x}_i) - y_i\\|^2 + \\alpha\\sum_{i=0}^{d-1}c_i^2.$$ \n",
    "Here ${\\bf x}_i$ are the data, $y_i$ are the labels, and $c_i$ are the coefficients to be solved.\n",
    "\n",
    "The regularization avoids the coefficients being too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(a)\n",
    "When `regularization==\"L1\"`, the correct gradient is `g = g0 + alpha * np.sign(c)` , where `g0` is the gradient when `regularization==None` .  \n",
    "Update your code for L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(b)\n",
    "When `regularization==\"L2\"`, the correct gradient is `g = g0 + alpha * 2 * v` , where `g0` is the gradient when `regularization==None` .  \n",
    "Update your code for L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
